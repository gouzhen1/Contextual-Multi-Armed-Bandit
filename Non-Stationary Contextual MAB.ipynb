{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-stationary Contextual Multi-armed Bandit\n",
    "## Introduction\n",
    "\n",
    "- Contextual MAB aims to find the best policy given context features as side information.\n",
    "- One key assumption: the underlying relationship between context features and rewards are fixed or move very slowly.\n",
    "\n",
    "### Problem\n",
    "- In many real world situations, this is not the case. \n",
    "- Factors can change suddenly and dramatically (non-stationary)\n",
    "\n",
    "### Examples\n",
    "- a user's interest in watching the NBA would suddenly drop in June when the season is over. \n",
    "- a user moved to another city or country.\n",
    "\n",
    "Therefore, in these situations, many alogrithms will be stuck in sub-optimal decisions for a long-time or forever.\n",
    "\n",
    "### Demonstration\n",
    "This notebook implemented a popular contextual bandit algorithm: LinUCB\n",
    "And applied this algorithm in both a stationary and a non-stationary environment.\n",
    "The result plots can be found at very bottom.\n",
    "We can see that in this simulation setup, given a stationary environment, our policy converges to optimal in about 20 timesteps. However, given the same setup, and we make a sudden change in the environment half-way. It takes almost 60 timestamp to converge again, and along the way accumulated large regrets. This is because after the first convergence, our model already has strong belief and it's much harder and slower to update.\n",
    "\n",
    "\n",
    "### Solutions\n",
    "We will focus on one of the solutions to this problem. \n",
    "\n",
    "The dLinUCB algorithm proposed by Wu et al in \n",
    "__[Learning Contextual Bandits in a Non-stationary Environment](https://arxiv.org/pdf/1805.09365.pdf)__ \n",
    "\n",
    "\n",
    "## Background\n",
    "One of the popular algorithms for contextual bandit is the LinUCB.\n",
    "\n",
    "$E[r_{t,a}|x_{t,a}] = x_{t,a}^{T}\\theta_{a}^{*}$\n",
    "\n",
    "x is context features, $\\theta$ is weights. So assumes the expected reward for an action under given context is a linear combination of features.\n",
    "\n",
    "Weights are calculated by a ridge regression over all the previous contexts encountered vs observed reward\n",
    "\n",
    "\n",
    "## References\n",
    "[1] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit\n",
    "approach to personalized news article recommendation. In Proceedings of\n",
    "19th WWW. ACM, 661â€“670.\n",
    "\n",
    "[2]Wu, Q., Iyer, N. and Wang, H., 2018. Learning Contextual Bandits in a Non-stationary Environment. arXiv preprint arXiv:1805.09365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "#Generate ground truth models\n",
    "#Here we assume the reward is linear in the context.\n",
    "#each action has a theta which is the weights for the context feature vector\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base bandit class\n",
    "class Agent():\n",
    "    def __init__(self, n_feature, n_action):\n",
    "        self.thetas = np.zeros((n_action,n_feature))\n",
    "        self.n_a = n_action\n",
    "        self.n_f = n_feature\n",
    "    \n",
    "    def pick_action(self,x):\n",
    "        pass\n",
    "    \n",
    "    def update_model(self,x,a,r):\n",
    "        pass\n",
    "\n",
    "#random bandit\n",
    "class RandomBandit(Agent):\n",
    "    def __init__(self, n_feature, n_action):\n",
    "        Agent.__init__(self,n_feature, n_action)\n",
    "        \n",
    "    def pick_action(self,x):\n",
    "        return np.random.randint(0,self.n_a)\n",
    "\n",
    "#LinUCB\n",
    "class LinUCBBandit(Agent):\n",
    "    def __init__(self, n_feature, n_action):\n",
    "        Agent.__init__(self,n_feature, n_action)\n",
    "        self.Ds = [np.zeros(self.n_f).reshape(1,self.n_f)] * self.n_a #exp\n",
    "        self.Cs = [0] * self.n_a #rewards\n",
    "        \n",
    "    def pick_action(self,x):\n",
    "        delta = 0.3\n",
    "        max_E = np.NINF\n",
    "        best_a = 0\n",
    "        for a in range(self.n_a):\n",
    "            D = self.Ds[a]\n",
    "            A = np.dot(D.T,D) + np.identity(self.n_f)\n",
    "            A_inverse = inv(A)\n",
    "            alpha = 1. + np.sqrt(np.log(2./delta)/2.)\n",
    "            bound = alpha * np.sqrt(np.dot(np.dot(x.T, A_inverse), x))\n",
    "            E = np.dot(x.T,self.thetas[a]) + bound\n",
    "            if E > max_E:\n",
    "                max_E = E\n",
    "                best_a = a\n",
    "\n",
    "        return best_a\n",
    "    \n",
    "    def update_model(self,x,a,r):\n",
    "        d = len(x)\n",
    "        self.Ds[a] = np.append(self.Ds[a],[x],axis=0)\n",
    "        self.Cs[a] = np.append(self.Cs[a],r)\n",
    "        D = self.Ds[a]\n",
    "        C = self.Cs[a]\n",
    "        inverse = inv(np.dot(D.T, D) + np.identity(d))\n",
    "        new_theta = np.dot(np.dot(inverse,D.T), C)\n",
    "        self.thetas[a] = new_theta\n",
    "        \n",
    "#Thompson Sampling with Bayesian Regression\n",
    "class ContextualTSAgent(Agent):\n",
    "    def __init__(self, n_feature, n_action):\n",
    "        Agent.__init__(self,n_feature, n_action)\n",
    "        self.exp_dict = {} # experience dict for each action\n",
    "        \n",
    "    def pick_action(self,x):\n",
    "        r_list = []\n",
    "        for a in range(self.n_a):\n",
    "            r_list.append(self.get_reward_sample(x,a))\n",
    "        return np.argmax(r_list)\n",
    "    \n",
    "    def get_reward_sample(self,x,a):\n",
    "        r = 0.\n",
    "        if a not in self.exp_dict: \n",
    "            return r\n",
    "        #hard coded formula for 3 variables for now.\n",
    "        with pm.Model() as model:\n",
    "            pm.GLM.from_formula('r ~ x_0 + x_1 + x_2', data=self.exp_dict[a])\n",
    "            traces = pm.sample(draws=1, chains=1, tune=100, compute_convergence_checks=False)\n",
    "            params = traces[0]\n",
    "            r = x[0] * params['x_0'] + x[1] * params['x_1'] + x[2] * params['x_2'] + params['Intercept']\n",
    "        return r\n",
    "    \n",
    "    def update_model(self,x,a,r):\n",
    "        new_df = pd.DataFrame({**{'x_{}'.format(i): x[i] for i in range(len(x))}, \"r\": r},index=[0])\n",
    "        if a not in self.exp_dict:\n",
    "            self.exp_dict[a] = new_df\n",
    "        else:\n",
    "            self.exp_dict[a] = pd.concat([new_df, self.exp_dict[a]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that runs simulation and plot regrets\n",
    "class Experiment():\n",
    "    def __init__(self, iter, agent, n_f, n_a, non_stationary):\n",
    "        self.agent = agent\n",
    "        self.iterations = iter\n",
    "        self.non_stationary = non_stationary\n",
    "        self.n_feature = n_f\n",
    "        self.n_action = n_a\n",
    "        self.true_thetas = None\n",
    "        \n",
    "    def get_true_reward(self, x,a):\n",
    "        return np.dot(x,self.true_thetas[a]) + np.random.rand() * 0.5\n",
    "    \n",
    "    def get_rand_context(self):\n",
    "        return np.random.randint(0,5,(self.n_feature))\n",
    "    \n",
    "    def gen_true_thetas(self):\n",
    "        self.true_thetas = np.random.rand(self.n_action,self.n_feature) * 3. - 1.5\n",
    "        print('Ground truth thetas')\n",
    "        for a in range(self.n_action):\n",
    "            print('action ' + str(a) + ': ' + str(self.true_thetas[a]))\n",
    "    \n",
    "    def get_optimal_action(self,x):\n",
    "        return np.argmax([self.get_true_reward(x,a) for a in range(self.n_action)])\n",
    "    \n",
    "    def run(self):\n",
    "        print('\\n\\nExperiment Starts')\n",
    "        np.random.seed(0)\n",
    "        regret = 0.\n",
    "        regret_list = []\n",
    "        t_change = np.floor(self.iterations * 0.5)\n",
    "        self.gen_true_thetas()\n",
    "        for t in range(self.iterations):\n",
    "            context = self.get_rand_context()\n",
    "            a = self.agent.pick_action(context)\n",
    "            r = self.get_true_reward(context,a)\n",
    "            self.agent.update_model(context,a,r)\n",
    "            regret += abs(r - self.get_true_reward(context,self.get_optimal_action(context)))\n",
    "            regret_list.append(regret)\n",
    "\n",
    "            if self.non_stationary and t == t_change:\n",
    "                print('env changed!')\n",
    "                self.gen_true_thetas()\n",
    "\n",
    "        print('Estimated thetas')\n",
    "        for a in range(self.agent.n_a):\n",
    "            print('action ' + str(a) + ': ' + str(self.agent.thetas[a]))\n",
    "        \n",
    "        fig,ax = plt.subplots()\n",
    "        ax.set_title('Cumulative Regret' if not self.non_stationary else 'Cumulative Regret(Non-stationary Env)')\n",
    "        ax.set_xlabel('t')\n",
    "        ax.set_ylabel('regret')\n",
    "        ax.set_ylim(0.,400.)\n",
    "        ax.plot(regret_list, label=str(type(self.agent).__name__))\n",
    "        if self.non_stationary:\n",
    "            ax.axvline(x=t_change,linestyle=':',color='green')\n",
    "            ax.text(t_change,0.,'Env Change')\n",
    "        ax.legend()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experiment Starts\n",
      "Ground truth thetas\n",
      "action 0: [0.14644051 0.6455681  0.30829013]\n",
      "action 1: [ 0.13464955 -0.2290356   0.43768234]\n",
      "action 2: [-0.18723837  1.175319    1.39098828]\n",
      "action 3: [-0.34967544  0.87517511  0.08668476]\n",
      "action 4: [ 0.20413368  1.27678991 -1.28689183]\n",
      "Estimated thetas\n",
      "action 0: [0.13250075 0.57163241 0.42525519]\n",
      "action 1: [ 0.16078104 -0.17650205  0.        ]\n",
      "action 2: [0.02325049 1.07407788 1.43098307]\n",
      "action 3: [0.22717899 0.15145266 0.15145266]\n",
      "action 4: [0.15951095 1.30744332 0.        ]\n",
      "\n",
      "\n",
      "Experiment Starts\n",
      "Ground truth thetas\n",
      "action 0: [0.14644051 0.6455681  0.30829013]\n",
      "action 1: [ 0.13464955 -0.2290356   0.43768234]\n",
      "action 2: [-0.18723837  1.175319    1.39098828]\n",
      "action 3: [-0.34967544  0.87517511  0.08668476]\n",
      "action 4: [ 0.20413368  1.27678991 -1.28689183]\n",
      "env changed!\n",
      "Ground truth thetas\n",
      "action 0: [-1.14381684 -0.54605046 -0.25721102]\n",
      "action 1: [-1.30755751  0.57741636  0.19980436]\n",
      "action 2: [-0.70383153  0.06974416 -1.21817847]\n",
      "action 3: [ 0.22783949  1.28788859 -0.54429314]\n",
      "action 4: [ 0.50223114 -1.10460641  0.64898161]\n",
      "Estimated thetas\n",
      "action 0: [0.13250075 0.57163241 0.42525519]\n",
      "action 1: [0.17254784 0.         0.        ]\n",
      "action 2: [-0.39694398  1.72819638 -0.69912895]\n",
      "action 3: [0.22717899 0.15145266 0.15145266]\n",
      "action 4: [-0.22938864 -0.30585152 -0.07646288]\n",
      "\n",
      "\n",
      "Experiment Starts\n",
      "Ground truth thetas\n",
      "action 0: [0.14644051 0.6455681  0.30829013]\n",
      "action 1: [ 0.13464955 -0.2290356   0.43768234]\n",
      "action 2: [-0.18723837  1.175319    1.39098828]\n",
      "action 3: [-0.34967544  0.87517511  0.08668476]\n",
      "action 4: [ 0.20413368  1.27678991 -1.28689183]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 55.66it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 62.18it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 58.54it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 62.78it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 81.00it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:02<00:00, 49.22it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 56.76it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:01<00:00, 70.47it/s]\n",
      "Only 1 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd, x_2, x_1, x_0, Intercept]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/101 [00:00<00:01, 56.63it/s]"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "ITERATIONS = 12\n",
    "FEATURES = 3\n",
    "ACTIONS = 5\n",
    "exp1 = Experiment(ITERATIONS, LinUCBBandit(FEATURES, ACTIONS),FEATURES, ACTIONS, False)\n",
    "exp2 =  Experiment(ITERATIONS, LinUCBBandit(FEATURES, ACTIONS),FEATURES, ACTIONS, True)\n",
    "exp1.run()\n",
    "exp2.run()\n",
    "agent = ContextualTSAgent(FEATURES, ACTIONS)\n",
    "exp3 = Experiment(ITERATIONS,agent ,FEATURES, ACTIONS, False)\n",
    "exp3.run()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
